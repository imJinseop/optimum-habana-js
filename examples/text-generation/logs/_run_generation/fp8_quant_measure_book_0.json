/usr/lib/python3.10/inspect.py:288: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead
  return isinstance(object, types.FunctionType)
/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
12/23/2024 07:15:31 - INFO - __main__ - Single-device run.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  6.56it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  7.10it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  7.31it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  7.58it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  7.39it/s]
2024-12-23 07:15:33 [WARNING][auto_accelerator.py:422] Auto detect accelerator: HPU_Accelerator.
2024-12-23 07:15:33 [INFO][utils.py:209] Preparation started.
2024-12-23 07:15:33 [INFO][quantize.py:160] Start to prepare model with fp8_quant.
============================= HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 1
 PT_RECIPE_CACHE_PATH = 
 PT_CACHE_FOLDER_DELETE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = 
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 2113399844 KB
------------------------------------------------------------------------------
2024-12-23 07:15:41 [INFO][utils.py:209] Preparation end.
12/23/2024 07:15:42 - INFO - __main__ - Args: Namespace(device='hpu', model_name_or_path='/model_weights/meta-llama/Llama-3.1-8B-Instruct/', bf16=True, max_new_tokens=100, max_input_tokens=200, batch_size=1, warmup=3, n_iterations=5, local_rank=0, use_kv_cache=True, use_hpu_graphs=True, dataset_name=None, column_name=None, do_sample=False, num_beams=1, top_k=None, penalty_alpha=None, trim_logits=False, seed=27, profiling_warmup_steps=0, profiling_steps=0, profiling_record_shapes=False, prompt=None, bad_words=None, force_words=None, assistant_model=None, peft_model=None, num_return_sequences=1, token=None, model_revision='main', attn_softmax_bf16=False, output_dir=None, bucket_size=-1, bucket_internal=False, dataset_max_samples=-1, limit_hpu_graphs=False, show_graphs_count=False, reuse_cache=False, verbose_workers=False, simulate_dyn_prompt=None, reduce_recompile=False, use_chat_template=False, use_flash_attention=True, flash_attention_recompute=False, flash_attention_causal_mask=False, flash_attention_fast_softmax=True, book_source=True, torch_compile=False, ignore_eos=True, temperature=1.0, top_p=1.0, const_serialization_path=None, trust_remote_code=False, parallel_strategy='none', input_embeds=False, run_partial_dataset=False, load_quantized_model_with_autogptq=False, disk_offload=False, load_quantized_model_with_inc=False, local_quantized_inc_model_path=None, quant_config='./quantization_config/maxabs_measure.json', world_size=0, global_rank=0)
12/23/2024 07:15:42 - INFO - __main__ - device: hpu, n_hpu: 0, bf16: True
12/23/2024 07:15:42 - INFO - __main__ - Model initialization took 12.025s
12/23/2024 07:15:44 - INFO - __main__ - Graph compilation...
Initializing inference mode
Book downloaded and saved to: /tmp/2701_51961.txt
Warming up iteration 1/3
/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
12/23/2024 07:15:46 - INFO - __main__ - Time to first token = 591.175390407443ms
Warming up iteration 2/3
12/23/2024 07:15:47 - INFO - __main__ - Time to first token = 37.6746691763401ms
Warming up iteration 3/3
12/23/2024 07:15:48 - INFO - __main__ - Time to first token = 32.270860858261585ms
12/23/2024 07:15:48 - INFO - __main__ - Running generate...
12/23/2024 07:15:49 - INFO - __main__ - Time to first token = 32.224311493337154ms
12/23/2024 07:15:50 - INFO - __main__ - Time to first token = 32.07683190703392ms
12/23/2024 07:15:50 - INFO - __main__ - Time to first token = 31.97169117629528ms
12/23/2024 07:15:51 - INFO - __main__ - Time to first token = 31.9299204275012ms
12/23/2024 07:15:52 - INFO - __main__ - Time to first token = 32.06694405525923ms

Input/outputs:
input 1: ('\ufeffThe Project Gutenberg eBook of Moby Dick; Or, The Whale This ebook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this ebook or online at www.gutenberg.org. If you are not located in the United States, you will have to check the laws of the country where you are located before using this eBook. Title: Moby Dick; Or, The Whale Author: Herman Melville Release date: July 1, 2001 [eBook #2701] Most recently updated: August 18, 2021 Language: English Credits: Daniel Lazarus, Jonesey, and David Widger *** START OF THE PROJECT GUTENBERG EBOOK MOBY DICK; OR, THE WHALE *** MOBY-DICK; or, THE WHALE. By Herman Melville CONTENTS ETYMOLOGY. EXTRACTS (Supplied by a Sub-Sub-Librarian). CHAPTER 1. Loomings. CHAPTER 2. The Carpet-Bag. CHAPTER 3. The Spouter-Inn. CHAPTER 4. The Counterpane. CHAPTER 5. Breakfast. CHAPTER 6. The Street. CHAPTER 7. The Chapel. CHAPTER 8. The Pulpit. CHAPTER 9. The Sermon. CHAPTER 10. A Bosom Friend. CHAPTER 11. Nightgown. CHAPTER 12. Biographical. CHAPTER 13. Wheelbarrow. CHAPTER 14. ',)
output 1.1: ('\ufeffThe Project Gutenberg eBook of Moby Dick; Or, The Whale This ebook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this ebook or online at www.gutenberg.org. If you are not located in the United States, you will have to check the laws of the country where you are located before using this eBook. Title: Moby Dick; Or, The Whale Author: Herman Melville Release date: July 1, 2001 [eBook #2701] Most recently updated: August 18, 2021 Language: English Credits: Daniel Lazarus, Jonesey, and David Widger *** START OF THE PROJECT GUTENBERG EBOOK MOBY DICK; OR, THE WHALE *** MOBY-DICK; or, THE WHALE. By HERMAN MELVILLE. CHAPTER 1. Looming white upon the sapphire waters, there lay a whiteness that was not of the whale, but of the snow. The ship, a whaler, had been some time in the Pacific, and was now sailing through a sea of ice, in the direction of the pole. The crew were all busy, and the captain was standing on the quarter-deck, looking out to sea. He was a tall, thin man,',)


Stats:
-----------------------------------------------------------------------------------
Input tokens
Throughput (including tokenization) = 112.09134781046089 tokens/second
Memory allocated                    = 16.25 GB
Max memory allocated                = 17.04 GB
Total memory available              = 94.62 GB
Graph compilation duration          = 3.7549503529444337 seconds
-----------------------------------------------------------------------------------

