/usr/lib/python3.10/inspect.py:288: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead
  return isinstance(object, types.FunctionType)
/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Fetching 19 files:   0%|          | 0/19 [00:00<?, ?it/s]Fetching 19 files: 100%|██████████| 19/19 [00:00<00:00, 17816.18it/s]
Fetching 19 files:   0%|          | 0/19 [00:00<?, ?it/s]Fetching 19 files: 100%|██████████| 19/19 [00:00<00:00, 15827.56it/s]
01/09/2025 09:39:28 - INFO - __main__ - Single-device run.
`LlamaRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/19 [00:00<00:11,  1.51it/s]Loading checkpoint shards:  11%|█         | 2/19 [00:01<00:11,  1.53it/s]Loading checkpoint shards:  16%|█▌        | 3/19 [00:01<00:10,  1.53it/s]Loading checkpoint shards:  21%|██        | 4/19 [00:02<00:09,  1.54it/s]Loading checkpoint shards:  26%|██▋       | 5/19 [00:03<00:09,  1.54it/s]Loading checkpoint shards:  32%|███▏      | 6/19 [00:03<00:08,  1.55it/s]Loading checkpoint shards:  37%|███▋      | 7/19 [00:04<00:07,  1.55it/s]Loading checkpoint shards:  42%|████▏     | 8/19 [00:05<00:07,  1.55it/s]Loading checkpoint shards:  47%|████▋     | 9/19 [00:05<00:06,  1.55it/s]Loading checkpoint shards:  53%|█████▎    | 10/19 [00:06<00:05,  1.56it/s]Loading checkpoint shards:  58%|█████▊    | 11/19 [00:07<00:05,  1.55it/s]Loading checkpoint shards:  63%|██████▎   | 12/19 [00:07<00:04,  1.55it/s]Loading checkpoint shards:  68%|██████▊   | 13/19 [00:08<00:03,  1.55it/s]Loading checkpoint shards:  74%|███████▎  | 14/19 [00:09<00:03,  1.56it/s]Loading checkpoint shards:  79%|███████▉  | 15/19 [00:09<00:02,  1.55it/s]Loading checkpoint shards:  84%|████████▍ | 16/19 [00:10<00:01,  1.56it/s]Loading checkpoint shards:  89%|████████▉ | 17/19 [00:10<00:01,  1.56it/s]Loading checkpoint shards:  95%|█████████▍| 18/19 [00:11<00:00,  1.56it/s]Loading checkpoint shards: 100%|██████████| 19/19 [00:12<00:00,  1.62it/s]Loading checkpoint shards: 100%|██████████| 19/19 [00:12<00:00,  1.56it/s]
============================= HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 1
 PT_RECIPE_CACHE_PATH = 
 PT_CACHE_FOLDER_DELETE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = 
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 2113399844 KB
------------------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/jinseop/optimum-habana-js/examples/text-generation/run_generation.py", line 779, in <module>
    main()
  File "/home/jinseop/optimum-habana-js/examples/text-generation/run_generation.py", line 385, in main
    model, assistant_model, tokenizer, generation_config = initialize_model(args, logger)
  File "/home/jinseop/optimum-habana-js/examples/text-generation/utils.py", line 720, in initialize_model
    setup_model(args, model_dtype, model_kwargs, logger)
  File "/home/jinseop/optimum-habana-js/examples/text-generation/utils.py", line 297, in setup_model
    model = model.eval().to(args.device)
  File "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py", line 2958, in to
    return super().to(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1177, in to
    return self._apply(convert)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  [Previous line repeated 4 more times]
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1163, in convert
    return t.to(
RuntimeError: [Rank:0] FATAL ERROR :: MODULE:PT_DEVMEM Allocation failed for size::234881024 (224)MB
