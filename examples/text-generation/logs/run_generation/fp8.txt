/usr/lib/python3.10/inspect.py:288: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead
  return isinstance(object, types.FunctionType)
/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
12/25/2024 13:50:49 - INFO - __main__ - Single-device run.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  6.45it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  7.05it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  7.22it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  7.52it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  7.31it/s]
2024-12-25 13:50:51 [WARNING][auto_accelerator.py:422] Auto detect accelerator: HPU_Accelerator.
2024-12-25 13:50:51 [INFO][utils.py:211] Conversion started.
2024-12-25 13:50:51 [INFO][quantize.py:224] Start to convert model with fp8_quant.
============================= HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 1
 PT_RECIPE_CACHE_PATH = 
 PT_CACHE_FOLDER_DELETE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = 
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 2113399844 KB
------------------------------------------------------------------------------
2024-12-25 13:50:58 [INFO][utils.py:211] Conversion end.
12/25/2024 13:51:00 - INFO - __main__ - Args: Namespace(device='hpu', model_name_or_path='/model_weights/meta-llama/Llama-3.1-8B-Instruct/', bf16=True, max_new_tokens=100, max_input_tokens=100, batch_size=1, warmup=3, n_iterations=5, local_rank=0, use_kv_cache=True, use_hpu_graphs=True, dataset_name=None, column_name=None, do_sample=False, num_beams=1, top_k=None, penalty_alpha=None, trim_logits=False, seed=27, profiling_warmup_steps=0, profiling_steps=0, profiling_record_shapes=False, prompt=None, bad_words=None, force_words=None, assistant_model=None, peft_model=None, num_return_sequences=1, token=None, model_revision='main', attn_softmax_bf16=False, output_dir=None, bucket_size=-1, bucket_internal=False, dataset_max_samples=-1, limit_hpu_graphs=False, show_graphs_count=False, reuse_cache=False, verbose_workers=False, simulate_dyn_prompt=None, reduce_recompile=False, use_chat_template=False, use_flash_attention=True, flash_attention_recompute=False, flash_attention_causal_mask=False, flash_attention_fast_softmax=True, book_source=True, torch_compile=False, ignore_eos=True, temperature=1.0, top_p=1.0, const_serialization_path=None, trust_remote_code=False, parallel_strategy='none', input_embeds=False, run_partial_dataset=False, load_quantized_model_with_autogptq=False, disk_offload=False, load_quantized_model_with_inc=False, local_quantized_inc_model_path=None, quant_config='./quantization_config/maxabs_quant.json', world_size=0, global_rank=0)
12/25/2024 13:51:00 - INFO - __main__ - device: hpu, n_hpu: 0, bf16: True
12/25/2024 13:51:00 - INFO - __main__ - Model initialization took 12.031s
12/25/2024 13:51:02 - INFO - __main__ - Graph compilation...
Initializing inference mode
Book downloaded and saved to: /tmp/2701_170193.txt
Warming up iteration 1/3
/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
12/25/2024 13:51:10 - INFO - __main__ - Time to first token = 3764.296523295343ms
Warming up iteration 2/3
12/25/2024 13:51:10 - INFO - __main__ - Time to first token = 15.599721111357212ms
Warming up iteration 3/3
12/25/2024 13:51:11 - INFO - __main__ - Time to first token = 10.581866838037968ms
12/25/2024 13:51:11 - INFO - __main__ - Running generate...
12/25/2024 13:51:11 - INFO - __main__ - Time to first token = 10.492078959941864ms
12/25/2024 13:51:12 - INFO - __main__ - Time to first token = 10.367526672780514ms
12/25/2024 13:51:12 - INFO - __main__ - Time to first token = 10.279337875545025ms
12/25/2024 13:51:13 - INFO - __main__ - Time to first token = 10.270779952406883ms
12/25/2024 13:51:13 - INFO - __main__ - Time to first token = 10.24566125124693ms

Input/outputs:
input 1: ('\ufeffThe Project Gutenberg eBook of Moby Dick; Or, The Whale This ebook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this ebook or online at www.gutenberg.org. If you are not located in the United States, you will have to check the laws of the country where you are located before using this eBook. Title: Moby Dick; Or, The Whale Author: Herman ',)
output 1.1: ('\ufeffThe Project Gutenberg eBook of Moby Dick; Or, The Whale This ebook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this ebook or online at www.gutenberg.org. If you are not located in the United States, you will have to check the laws of the country where you are located to determine whether your use of this ebook is legal under those laws. Title: Moby Dick; Or, The Whale Author: Herman Melville Translator: N/A Illustrator: N/A Release Date: March 2004 [EBook #3744] Language: English Character set encoding: ISO-8859-1 *** START OF THIS PROJECT GUTENBERB *** Produced by David Price, and David Widger *** Produced by David Price, and David Widger from page images generously made',)


Stats:
-----------------------------------------------------------------------------------
Input tokens
Throughput (including tokenization) = 205.35888067166263 tokens/second
Memory allocated                    = 8.22 GB
Max memory allocated                = 9.13 GB
Total memory available              = 94.62 GB
Graph compilation duration          = 8.528509501367807 seconds
-----------------------------------------------------------------------------------

