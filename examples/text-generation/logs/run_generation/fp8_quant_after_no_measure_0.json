/usr/lib/python3.10/inspect.py:288: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead
  return isinstance(object, types.FunctionType)
/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
12/23/2024 07:13:14 - INFO - __main__ - Single-device run.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  6.67it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  7.18it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  7.25it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  7.59it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  7.40it/s]
2024-12-23 07:13:16 [WARNING][auto_accelerator.py:422] Auto detect accelerator: HPU_Accelerator.
2024-12-23 07:13:16 [INFO][utils.py:211] Conversion started.
2024-12-23 07:13:16 [INFO][quantize.py:224] Start to convert model with fp8_quant.
============================= HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 1
 PT_RECIPE_CACHE_PATH = 
 PT_CACHE_FOLDER_DELETE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = 
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 2113399844 KB
------------------------------------------------------------------------------
2024-12-23 07:13:23 [INFO][utils.py:211] Conversion end.
12/23/2024 07:13:25 - INFO - __main__ - Args: Namespace(device='hpu', model_name_or_path='/model_weights/meta-llama/Llama-3.1-8B-Instruct/', bf16=True, max_new_tokens=100, max_input_tokens=200, batch_size=1, warmup=3, n_iterations=5, local_rank=0, use_kv_cache=True, use_hpu_graphs=True, dataset_name=None, column_name=None, do_sample=False, num_beams=1, top_k=None, penalty_alpha=None, trim_logits=False, seed=27, profiling_warmup_steps=0, profiling_steps=0, profiling_record_shapes=False, prompt=None, bad_words=None, force_words=None, assistant_model=None, peft_model=None, num_return_sequences=1, token=None, model_revision='main', attn_softmax_bf16=False, output_dir=None, bucket_size=-1, bucket_internal=False, dataset_max_samples=-1, limit_hpu_graphs=False, show_graphs_count=False, reuse_cache=False, verbose_workers=False, simulate_dyn_prompt=None, reduce_recompile=False, use_chat_template=False, use_flash_attention=True, flash_attention_recompute=False, flash_attention_causal_mask=False, flash_attention_fast_softmax=True, book_source=True, torch_compile=False, ignore_eos=True, temperature=1.0, top_p=1.0, const_serialization_path=None, trust_remote_code=False, parallel_strategy='none', input_embeds=False, run_partial_dataset=False, load_quantized_model_with_autogptq=False, disk_offload=False, load_quantized_model_with_inc=False, local_quantized_inc_model_path=None, quant_config='./quantization_config/maxabs_quant.json', world_size=0, global_rank=0)
12/23/2024 07:13:25 - INFO - __main__ - device: hpu, n_hpu: 0, bf16: True
12/23/2024 07:13:25 - INFO - __main__ - Model initialization took 12.004s
12/23/2024 07:13:28 - INFO - __main__ - Graph compilation...
Initializing inference mode
Book downloaded and saved to: /tmp/2701_49373.txt
Warming up iteration 1/3
/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
12/23/2024 07:13:35 - INFO - __main__ - Time to first token = 3771.10736630857ms
Warming up iteration 2/3
12/23/2024 07:13:36 - INFO - __main__ - Time to first token = 21.80612925440073ms
Warming up iteration 3/3
12/23/2024 07:13:36 - INFO - __main__ - Time to first token = 16.797900199890137ms
12/23/2024 07:13:36 - INFO - __main__ - Running generate...
12/23/2024 07:13:37 - INFO - __main__ - Time to first token = 16.63130521774292ms
12/23/2024 07:13:37 - INFO - __main__ - Time to first token = 16.546069644391537ms
12/23/2024 07:13:38 - INFO - __main__ - Time to first token = 16.49115514010191ms
12/23/2024 07:13:38 - INFO - __main__ - Time to first token = 16.5406446903944ms
12/23/2024 07:13:39 - INFO - __main__ - Time to first token = 16.486051492393017ms

Input/outputs:
input 1: ('\ufeffThe Project Gutenberg eBook of Moby Dick; Or, The Whale This ebook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this ebook or online at www.gutenberg.org. If you are not located in the United States, you will have to check the laws of the country where you are located before using this eBook. Title: Moby Dick; Or, The Whale Author: Herman Melville Release date: July 1, 2001 [eBook #2701] Most recently updated: August 18, 2021 Language: English Credits: Daniel Lazarus, Jonesey, and David Widger *** START OF THE PROJECT GUTENBERG EBOOK MOBY DICK; OR, THE WHALE *** MOBY-DICK; or, THE WHALE. By Herman Melville CONTENTS ETYMOLOGY. EXTRACTS (Supplied by a Sub-Sub-Librarian). CHAPTER 1. Loomings. CHAPTER 2. The Carpet-Bag. CHAPTER 3. The Spouter-Inn. CHAPTER 4. The Counterpane. CHAPTER 5. Breakfast. CHAPTER 6. The Street. CHAPTER 7. The Chapel. CHAPTER 8. The Pulpit. CHAPTER 9. The Sermon. CHAPTER 10. A Bosom Friend. CHAPTER 11. Nightgown. CHAPTER 12. Biographical. CHAPTER 13. Wheelbarrow. CHAPTER 14. ',)
output 1.1: ('\ufeffThe Project Gutenberg eBook of Moby Dick; Or, The Whale This ebook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this ebook or online at www.gutenberg.org. If you are not located in the United States, you will have to check the laws of the country where you are located before using this eBook. Title: Moby Dick; Or, The Whale Author: Herman Melville Release date: July 1, 2001 [eBook #2701] Most recently updated: August 18, 2021 Language: English Credits: Daniel Lazarus, Jonesey, and David Widger *** START OF THE PROJECT GUTENBERG EBOOK MOBY DICK; OR, THE WHALE *** MOBY-DICK; or, THE WHALE. By HERMAN MELVILLE. CHAPTER 1. Loomings. Call me Ishmael. Some years ago—never mind how long precisely—having little or no money in my purse, and nothing particular to interest me on shore, I thought I would sail about a little and see the watery part of the world. It is a way I have of driving off the spleen and regulating the circulation. Whenever I find myself growing grim about the mouth; whenever it is a damp, dr',)


Stats:
-----------------------------------------------------------------------------------
Input tokens
Throughput (including tokenization) = 199.58966431844766 tokens/second
Memory allocated                    = 8.29 GB
Max memory allocated                = 9.14 GB
Total memory available              = 94.62 GB
Graph compilation duration          = 8.578234040178359 seconds
-----------------------------------------------------------------------------------

